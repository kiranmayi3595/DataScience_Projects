{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndOsvItYTg-R"
   },
   "source": [
    "#**Final submission-SARIMA forecast**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEDoTTnbuiRF"
   },
   "outputs": [],
   "source": [
    "# It is recommended to upgrade the statsmodels library. \n",
    "# Uncomment the below code to upgrade statsmodels\n",
    "!pip install statsmodels --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to ignore warnings\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6y-aVmMwZaIb",
    "outputId": "533e578e-12fb-4eb8-b7b0-25596449f067",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('MER_T12_06.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dixG2IyfPK4"
   },
   "outputs": [],
   "source": [
    "#conversion of \"YYYYMM\" columnn into standard datetime format & making it as index\n",
    "# We are using errors=’coerce’. It will replace all non-numeric values with NaN.\n",
    "\n",
    "dateparse = lambda x: pd.to_datetime(x, format='%Y%m', errors = 'coerce')\n",
    "df = pd.read_excel('MER_T12_06.xlsx', parse_dates=['YYYYMM'], index_col='YYYYMM', date_parser=dateparse) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "498vCfjeU8Sn"
   },
   "source": [
    "**The arguments can be explained as:**\n",
    "\n",
    "- **parse_dates:** This is a key to identify the date time column. Example, the column name is ‘YYYYMM’.\n",
    "- **index_col:** This is a key that forces pandas to use the date time column as index.\n",
    "- **date_parser:** Converts an input string into datetime variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2ssttk5XZsL"
   },
   "source": [
    "- Let us first identify and **drop the non datetimeindex** rows. First, let's convert the index to datetime, coerce errors, and filter NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "FZWK9U_QfQ7d",
    "outputId": "3701c923-da24-48f2-92e3-221933b2a4e1"
   },
   "outputs": [],
   "source": [
    "ts = df[pd.Series(pd.to_datetime(df.index, errors='coerce')).notnull().values]\n",
    "ts.head()\n",
    "ts.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The observations have reduced to 4707 after filtering on NaT\n",
    "2. There are 9 unique categories in MSN and Description columns\n",
    "3. The 'Value' coulmn has missing values with a high frequency of 384. The rows with these missing values should be eliminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the emision value into numeric value\n",
    "nat=pd.DataFrame(pd.to_numeric(ts['Value'],errors='coerce')).convert_dtypes()\n",
    "ts['Value']=nat['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the missing value using dropna(inplace = True)\n",
    "ts.dropna(inplace = True)\n",
    "ts.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImKq12UQ-gtC"
   },
   "source": [
    "### **Natural gas based CO2 emission forecasting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1ztc4I8_dYD"
   },
   "source": [
    "For developing the time series model and forecasting, you are expected to use the natural gas CO2 emission from the electrical power generation. We need to slice this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0nzeIaA_cZ9"
   },
   "outputs": [],
   "source": [
    "###Slice the data to get the monthly total CO2 emissions of Natural Gas Electric Power Sector\n",
    "natural=ts[ts['MSN']=='NNEIEUS']\n",
    "natural= pd.DataFrame(natural).drop(['Description','MSN'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIFzFIwa9gmN"
   },
   "outputs": [],
   "source": [
    " #Check 1st few rows of data\n",
    "natural.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5o58ikzZ4VJ"
   },
   "source": [
    "# **Split the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntpQTlJJZykI"
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "# using first 41 years data as training data\n",
    "train_data = natural.loc['1973-01-01':'2014-01-01']\n",
    "\n",
    "# using the last 2 years data as test data\n",
    "test_data = natural.loc['2014-01-01':'2016-07-01']\n",
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoI_KXuOBgcS"
   },
   "source": [
    "###**Test the Stationarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s76c_hzK_Sg3"
   },
   "outputs": [],
   "source": [
    "#Import the required package\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DDKCUtCB59A"
   },
   "source": [
    "###**Test the stationarity through Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a subplot space\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# plotting train data\n",
    "train_data.plot(ax=ax)\n",
    "\n",
    "# plotting test data\n",
    "test_data.plot(ax=ax)\n",
    "\n",
    "# adding the legends in sequential order\n",
    "plt.legend(['train data', 'test data'])\n",
    "\n",
    "# showing the time which divides the original data into train and test\n",
    "plt.axvline(x='2014-01-01', color='black', linestyle='--')\n",
    "\n",
    "# showing the plot\n",
    "plt.show()\n",
    "fig.savefig(\"series\", bbox_inches='tight', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bt4y7zusBv5b"
   },
   "outputs": [],
   "source": [
    "# Calculate the rolling mean and standard deviation for a window of 12 observations\n",
    "rolmean=train_data.rolling(window=12).mean()\n",
    "rolstd=train_data.rolling(window=12).std()\n",
    "\n",
    "# Visualize the rolling mean and standard deviation\n",
    "fig2=plt.figure(figsize=(16,8))\n",
    "actual = plt.plot(train_data, color='cyan', label='Actual Series')\n",
    "rollingmean = plt.plot(rolmean, color='red', label='Rolling Mean') \n",
    "rollingstd = plt.plot(rolstd, color='green', label='Rolling Std. Dev.')\n",
    "plt.title('Rolling Mean & Standard Deviation of the Series')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig2.savefig(\"Rolling_mean\", bbox_inches='tight', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnwqnEeDDxUh"
   },
   "source": [
    "#### **Observations and Insights: ____**\n",
    "1. Series has upward trend, it is not stationary\n",
    "2. The rolling mean does a good job in capturing the avergae trend of the data. However, it fails to capture seasonal patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kbkBI8HEFJe"
   },
   "source": [
    "# **Test the stationarity using the Augmented Dickey-Fuller Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQr0yVIKlAp5"
   },
   "source": [
    "Use the **Augmented Dickey-Fuller (ADF) Test** to verify if the series is stationary or not. The null and alternate hypotheses for the ADF Test are defined as:\n",
    "\n",
    "**- Null hypothesis:** The Time Series is non-stationary\n",
    "\n",
    "\n",
    "**- Alternative hypothesis:** The Time Series is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to use adfuller test\n",
    "def adfuller(train_data):\n",
    "  #Importing adfuller using statsmodels\n",
    "  from statsmodels.tsa.stattools import adfuller\n",
    "  print('Dickey-Fuller Test: ')\n",
    "  adftest = adfuller(train_data['Value'])\n",
    "  adfoutput = pd.Series(adftest[0:4], index=['Test Statistic','p-value','Lags Used','No. of Observations'])\n",
    "  for key,value in adftest[4].items():\n",
    "    adfoutput['Critical Value (%s)'%key] = value\n",
    "  print(adfoutput)\n",
    "adfuller(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjwqnZVGJnC8"
   },
   "source": [
    "- **Observations and Insights**\n",
    "- 1. From the above test, we can see that the p-value = 0.995 i.e. > 0.05 (For 95% confidence intervals) therefore, we fail to reject the null hypothesis.\n",
    "- 2. Hence, we can confirm that the series is non-stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu0r1kYWJ1rO"
   },
   "source": [
    "# **Transformation of the dataset into a stationary one**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9Wv-ufGnBRp"
   },
   "source": [
    "**We can use some of the following methods to convert a non-stationary series into a stationary one:**\n",
    "\n",
    "\n",
    "1. Log Transformation\n",
    "2. Differencing the series "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOW6NcU-VN02"
   },
   "source": [
    "We take the average of ‘k’ consecutive values depending on the frequency of time series (in this capstone 12 months). \n",
    "\n",
    "Here, we will take the average over the past 1 year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fE2MvqLJqi6"
   },
   "outputs": [],
   "source": [
    " # Visualize the rolling mean and standard deviation after using log transformation\n",
    "plt.figure(figsize=(16,8))\n",
    "df_log = np.log(train_data) #Reduced variance\n",
    "MAvg = df_log.rolling(window=12).mean()\n",
    "MStd = df_log.rolling(window=12).std()\n",
    "fig3=plt.plot(df_log)\n",
    "plt.plot(MAvg, color='r', label = 'Moving Average')\n",
    "plt.plot(MStd, color='g', label = 'Standard Deviation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25V8-kvKV7hN"
   },
   "source": [
    "**Observations and Insights: _____**\n",
    "- Since **we can still see the upward trend in the series**, we can conclude that **the series is still non-stationary.** \n",
    "- However, the standard deviation is almost constant which implies that **now the series has constant variance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlQacDVzpfrJ"
   },
   "source": [
    "**Visualize the rolling mean and rolling standard deviation of the shifted series (df_shift) and check the stationarity by calling the adfuller() function. Also, write your observations on the same.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "df_shift = df_log - df_log.shift(periods = 1,fill_value=0)\n",
    "MAvg_shift = df_shift.rolling(window=12).mean()\n",
    "MStd_shift = df_shift.rolling(window=12).std()\n",
    "plt.plot(df_shift, color='c')\n",
    "plt.plot(MAvg_shift, color='red', label = 'Moving Average')\n",
    "plt.plot(MStd_shift, color='green', label = 'Standard Deviation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Dropping the null values that we get after applying differencing method\n",
    "df_shift = df_shift.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations and Insights:___**\n",
    "- 1.Since we can no longer see an upward trend, the series seems to be almost constant (stationary)\n",
    "\n",
    "- 2. The standard deviation also seems to be almost constant\n",
    "\n",
    "**Lets verify using Augmented Dickey-Fuller (ADF) Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gD_Haj9zqD-M"
   },
   "outputs": [],
   "source": [
    "adfuller(df_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations and Insights:**\n",
    "1. From the above test, we can see that the p-value = 1.93e-8 i.e. < 0.05 (For 95% confidence intervals) therefore, we can reject the null hypothesis.\n",
    "2. Hence, we can confirm that the series is now stationary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtaining stationarity through seasonal differencing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "# implementing ADF test on the original time series data\n",
    "result = adfuller(train_data['Value'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "train_data.plot(ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# printing the results\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking seasonal differencing of the timeseries\n",
    "train_data_stationary = train_data.diff(periods=12).dropna()\n",
    "\n",
    "# implementing ADF test on the first order differenced time series data\n",
    "result = adfuller(train_data_stationary['Value'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "train_data_stationary.plot(ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# printing the results\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHIOPvYWqMe4"
   },
   "source": [
    "#### **Observations and Insights: _____**\n",
    "1. p-value=2.44e-6<0.05. Stationarity is obtained by 1st order differencing. Therefore D=1 in SARIMA modelling\n",
    "\n",
    "Let's decompose the time series to check its different components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtttBdemiIxY"
   },
   "source": [
    "### **Elimination of trend and seasonality: Decomposition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBBWQmpLh_ir"
   },
   "outputs": [],
   "source": [
    "#Importing the seasonal_decompose function to decompose the time series\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomp = seasonal_decompose(train_data)\n",
    "\n",
    "trend = decomp.trend\n",
    "seasonal = decomp.seasonal\n",
    "residual = decomp.resid\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(411)\n",
    "plt.plot(train_data, label='Actual', marker='.')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend', marker='.')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal, label='Seasonality', marker='.')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals', marker='.')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWUmcqeyjQff"
   },
   "source": [
    "**Observations and Insights: ____**\n",
    "- We can see that there are significant **trend, seasonality and residuals components** in the series\n",
    "- The plot for seasonality shows that **Natural gas based CO2 emissions spike in July and August.**\n",
    "\n",
    "**Now let's move on to the model building section. First, we will plot the `ACF` and `PACF` plots to get the values of p and q i.e. order of AR and MA models to be used.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iv1j-3V3rM92"
   },
   "source": [
    "**Plot the auto-correlation function and partial auto-correlation function to get p and q values for AR, MA, ARMA, and ARIMA models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtstH0CvKVMX"
   },
   "source": [
    "### **Find optimal parameters (p,d,q)x(P,D,Q) and build the SARIMA model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umZ3XhCyK40g"
   },
   "source": [
    "**Plot the ACF and PACF charts and find the optimal parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igReFApvKmhu"
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "\n",
    "plt.figure(figsize = (16,8))\n",
    "plot_acf(df_shift, lags = 24) \n",
    "plt.show() \n",
    "plot_pacf(df_shift, lags = 24) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXp8sqECya6e"
   },
   "source": [
    "**Observations and Insights: _____**\n",
    "\n",
    "**Observations:**\n",
    "- From the above PACF plot we can see that **the lag** at which the plot extends beyond the statistically significant boundary for the first time is **lag 1.** \n",
    "- This indicates that **lag 1 (p=1)** should be sufficient to fit the data.\n",
    "- We observed a periodicity on the PACF since the underlying data is periodic with a period of 12 months. Hence, 'P' which is the order of seasonal AR term in SARIMA model is equal to 1.\n",
    "\n",
    "- Similarly, from the ACF plot, we can infer that **q=1.**\n",
    "- The ACF and PACF also capture the seasonality in the data\n",
    "- We observed a periodicity on the ACF since the underlying data is periodic with a period of 12 months. Hence, 'Q' which is the order of seasonal MA term in SARIMA model is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Fit and predict the shifted series with the SARIMA Model and calculate the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the p, d and q parameters to take any value between 0 and 3\n",
    "p = d = q = range(0, 2)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "print('Examples of parameter combinations for Seasonal ARIMA: ')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determing p,d,q combinations with AIC scores.\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        mod = sm.tsa.statespace.SARIMAX(df_log.astype(float),\n",
    "                                        order=param,\n",
    "                                        seasonal_order=param_seasonal,\n",
    "                                        enforce_stationarity=False,enforce_invertibility=False)\n",
    "\n",
    "        results = mod.fit(disp=0)\n",
    "\n",
    "        print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "1. The (p,d,q)x(P,D,Q) combination that resulted in the least AIC score (-1048.0736) is (1,1,1)x(1,0,1,12). Therefore, this combination is a good starting point for fitting the SARIMA model on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting SARIMA model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating an SARIMA model with parameters (1, 1, 1)x(1, 0, 1, 12)\n",
    "mod = sm.tsa.statespace.SARIMAX(df_log.astype(float),\n",
    "                                order=(1, 1, 1),\n",
    "                                seasonal_order=(1, 0, 1, 12),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "results = mod.fit(disp=0)\n",
    "\n",
    "print(round(results.aic,2))\n",
    "print(results.summary().tables[1])\n",
    "results.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "Top left: The residual errors seem to fluctuate around a mean of zero.\n",
    "\n",
    "Top Right: The density plot suggests that the distribution of residuals is very close to a standard normal distribution.\n",
    "\n",
    "Bottom left: The quantile-quantile plot indicates that the sample quantiles are approximately equal to the theoritical quantiles. Any significant deviations would imply the distribution of residuals is skewed.\n",
    "\n",
    "Bottom Right: The ACF plot shows the residuals are not autocorrelated as no lag other than 0 is significant. Any autocorrelation would imply that there is some pattern in the residual errors which are not explained in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plots show that the model assumptions are not violated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inverse Transformation**\n",
    "\n",
    "**Use the correct inverse transformation depending on the model chosen to get back the original values**\n",
    "\n",
    "**Apply an inverse transformation on the predictions of the chosen model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the fitted values\n",
    "predictions=pd.Series(results.fittedvalues)\n",
    "#predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Third step - applying exponential transformation\n",
    "predictions_SARIMA = np.exp(predictions)#use exponential function\n",
    "#predictions_SARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the original vs predicted series of the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here\n",
    "#Plotting the original vs predicted series\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(train_data, color = 'c', label = 'Original Series')  #plot the original train series\n",
    "plt.plot(predictions_SARIMA, color = 'r', label = 'Predicted Series')  #plot the predictions_ARIMA \n",
    "plt.title('Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations & Insights**:  \n",
    "- We can see that the predicted series is very similar to the original series i.e. The model is good at predicting values on the training data, apart from one outlier prediction in 1974.**\n",
    "- The seasonal aspect of the SARIMA model helps us better capture the seasonal variations in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast from 2014 to 2016 to check if the predictions align with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the code blocks based on the requirements\n",
    "forecasted_SARIMA = results.forecast(steps=31)#forecast using the results_ARIMA for next 31 months. Keep steps=24\n",
    "forecasted_SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2 = forecasted_SARIMA.tolist()\n",
    "series2 = pd.Series(list2)\n",
    "#series2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.date_range('2013-12-01','2016-07-01' , freq='1M')\n",
    "df2 = pd.DataFrame()\n",
    "df2['forecasted'] = np.exp(series2)\n",
    "df2.index = index\n",
    "#df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the original vs predicted series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the original vs predicted series\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(train_data, color = 'c', label = 'Training data')\n",
    "plt.plot(predictions_SARIMA, color = 'r', label = 'Prediction on Train data') #plot the predictions_ARIMA series\n",
    "plt.plot(test_data,color='b',label='Test Data')\n",
    "plt.plot(df2, label = 'Forecast', color='g')  #plot the forecasted_ARIMA series\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's test the RMSE of the transformed predictions and the original value on the training and testing data to check whether the model is giving a generalized performance or not**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "error =np.sqrt(mean_squared_error(predictions_SARIMA, train_data)) #calculate RMSE using the predictions_ARIMA and df_train \n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "error = np.sqrt(mean_squared_error(forecasted_SARIMA, test_data))\n",
    "error#calculate RMSE using the forecasted_ARIMA and df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations and Insights**\n",
    "1. The above plot shows that the model is able to identify the trend and seasonalities in the data. \n",
    "2. The forecasted values are similar to the test data\n",
    "3. Based on the RMSE values and the plot, it looks like this model is able to capture the trend and seasonal patterns well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "1. The small RMSE values also indicate that this is a good model to forecast CO2 emission values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast for the next 24 months from original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating an SARIMA model with parameters (1, 1, 1)x(1, 0, 1, 12)\n",
    "df_log_original=np.log(natural)\n",
    "mod_original = sm.tsa.statespace.SARIMAX(df_log_original.astype(float),\n",
    "                                order=(1, 1, 1),\n",
    "                                seasonal_order=(1, 0, 1, 12),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "results_original = mod_original.fit(disp=0)\n",
    "\n",
    "print(round(results_original.aic,2))\n",
    "print(results_original.summary().tables[1])\n",
    "results_original.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the fitted values\n",
    "predictions_final=pd.Series(results_original.fittedvalues)\n",
    "#predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Third step - applying exponential transformation\n",
    "predictions_SARIMA_final = np.exp(predictions_final)#use exponential function\n",
    "#predictions_SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the code blocks based on the requirements\n",
    "forecasted_SARIMA_final = results_original.forecast(steps=48)#forecast using the results_ARIMA for next 24 months. Keep steps=24\n",
    "#forecasted_SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2 = forecasted_SARIMA_final.tolist()\n",
    "series2 = pd.Series(list2)\n",
    "#series2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.date_range('2016-07-02','2020-07-02' , freq='1M')- pd.offsets.MonthBegin(1)\n",
    "df3 = pd.DataFrame()\n",
    "df3['forecasted'] = np.exp(series2)\n",
    "df3.index = index\n",
    "#df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the original vs predicted series\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(natural, color = 'c', label = 'Original Series')\n",
    "plt.plot(df3, color = 'b', label = 'Forecast') #plot the predictions_ARIMA series\n",
    "plt.title('Original vs Forecast')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Executive Summary**\n",
    "1. Upon trying various models such as AR,MA, ARMA and ARIMA to fit the natural gas CO2 emissions data from 1973 to 2016, SARIMA model seems to fit the data very well.\n",
    "2. This model captures the trend as well as the seasonalities in the dtraining data and and fits well with the test data too\n",
    "3. The natural gas CO2 emissions forecast fot the next 24 months (2016 to 2018) indicates an upwards trend ans seasonliities eith spikes in July and August every year "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem and Solution Summary**\n",
    "- 1. The problem in question is to understand how the CO2 emissions due to natural gasconsumption would behave in the future.\n",
    "- 2. This would potentially help us in designing new energy efficient policies.\n",
    "- 3. Key solution design points:\n",
    "    - a. CO2 emissions due to natural gas consumptions in the last 40 years, displays trends and seasonal patterns.The forecasted data should be able to predict such trends and seasonal occurances.\n",
    "    - b. SARIMA model is able to capture trend and seasonalities. Hence, it is chosen for fitting the time series daata of CO2 emissions due to natural gas consumption\n",
    "    - The data was split into training data (1973-2014) and test data (2014-2016)\n",
    "    - c. The SARIMA model was tuned using grid search on the training data, and the metric used to select the best model was AIC. The best model was ARIMA(1,1,1)x(1,0,1,12), AIC: -1048\n",
    "    - d. Further, the performance of the model was evaluated by plotting the predicted values and comparing to that of the test data using RMSE\n",
    "    - f. Finally, the forecast for the next 48 months (2016-2020) was obtained using the chosen model. The forecast indicates upward trends and seasonalities with periodic spikes in July and AUgust of every year. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendations for Implementation**\n",
    "- 1. Co2 emissions maybe influenced by numerous other features in addition to using lagged values. Hence, a supplemental analysis done by identifying with stakeholders key drivers and improving model forecasts would be an immediate next step\n",
    "- 2. Forecasts may be used to assess environmental/health costs and used as a lever to justify increased investments in renewable energy like solar, wind etc\n",
    "- 3. A key risk in implementation is regional/geographical variation in Co2 behaviour. This study may need to be done acroos multiple geographies to justify investment in renewable sources\n",
    "- 4. A cost/benefit analysis maybe done using the forecasts. What is the cost of reducing natural gas CO2 emissions? Would the benefit gained by shifting to other sources outweigh the short term cost of shifting sources?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_Axk38bXXNcS",
    "9hY6TyAS7Pkj",
    "qIhA2bboDxiD"
   ],
   "name": "Reference_Notebook_Milestone_2_Time_Series.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
