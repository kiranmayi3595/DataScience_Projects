{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndOsvItYTg-R"
   },
   "source": [
    "#**Milestone 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEDoTTnbuiRF"
   },
   "outputs": [],
   "source": [
    "# It is recommended to upgrade the statsmodels library. \n",
    "# Uncomment the below code to upgrade statsmodels\n",
    "!pip install statsmodels --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to ignore warnings\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6y-aVmMwZaIb",
    "outputId": "533e578e-12fb-4eb8-b7b0-25596449f067",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('MER_T12_06.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dixG2IyfPK4"
   },
   "outputs": [],
   "source": [
    "#conversion of \"YYYYMM\" columnn into standard datetime format & making it as index\n",
    "# We are using errors=’coerce’. It will replace all non-numeric values with NaN.\n",
    "\n",
    "dateparse = lambda x: pd.to_datetime(x, format='%Y%m', errors = 'coerce')\n",
    "df = pd.read_excel('MER_T12_06.xlsx', parse_dates=['YYYYMM'], index_col='YYYYMM', date_parser=dateparse) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "498vCfjeU8Sn"
   },
   "source": [
    "**The arguments can be explained as:**\n",
    "\n",
    "- **parse_dates:** This is a key to identify the date time column. Example, the column name is ‘YYYYMM’.\n",
    "- **index_col:** This is a key that forces pandas to use the date time column as index.\n",
    "- **date_parser:** Converts an input string into datetime variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2ssttk5XZsL"
   },
   "source": [
    "- Let us first identify and **drop the non datetimeindex** rows. First, let's convert the index to datetime, coerce errors, and filter NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "FZWK9U_QfQ7d",
    "outputId": "3701c923-da24-48f2-92e3-221933b2a4e1"
   },
   "outputs": [],
   "source": [
    "ts = df[pd.Series(pd.to_datetime(df.index, errors='coerce')).notnull().values]\n",
    "ts.head()\n",
    "ts.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The observations have reduced to 4707 after filtering on NaT\n",
    "2. There are 9 unique categories in MSN and Description columns\n",
    "3. The 'Value' coulmn has missing values with a high frequency of 384. The rows with these missing values should be eliminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the emision value into numeric value\n",
    "nat=pd.DataFrame(pd.to_numeric(ts['Value'],errors='coerce')).convert_dtypes()\n",
    "ts['Value']=nat['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the missing value using dropna(inplace = True)\n",
    "ts.dropna(inplace = True)\n",
    "ts.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImKq12UQ-gtC"
   },
   "source": [
    "### **Natural gas based CO2 emission forecasting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1ztc4I8_dYD"
   },
   "source": [
    "For developing the time series model and forecasting, you are expected to use the natural gas CO2 emission from the electrical power generation. We need to slice this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0nzeIaA_cZ9"
   },
   "outputs": [],
   "source": [
    "###Slice the data to get the monthly total CO2 emissions of Natural Gas Electric Power Sector\n",
    "natural=ts[ts['MSN']=='NNEIEUS']\n",
    "natural= pd.DataFrame(natural).drop(['Description','MSN'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIFzFIwa9gmN"
   },
   "outputs": [],
   "source": [
    " #Check 1st few rows of data\n",
    "natural.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5o58ikzZ4VJ"
   },
   "source": [
    "###**Split the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntpQTlJJZykI"
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "# using first 34 years data as training data\n",
    "train_data = natural.loc['1973-01-01':'2007-11-01']\n",
    "\n",
    "# using the last 9 years data as test data\n",
    "test_data = natural.loc['2007-12-01':'2016-07-01']\n",
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoI_KXuOBgcS"
   },
   "source": [
    "###**Test the Stationarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s76c_hzK_Sg3"
   },
   "outputs": [],
   "source": [
    "#Import the required package\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import coint, adfuller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DDKCUtCB59A"
   },
   "source": [
    "###**Test the stationarity through Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a subplot space\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# plotting train data\n",
    "train_data.plot(ax=ax)\n",
    "\n",
    "# plotting test data\n",
    "test_data.plot(ax=ax)\n",
    "\n",
    "# adding the legends in sequential order\n",
    "plt.legend(['train data', 'test data'])\n",
    "\n",
    "# showing the time which divides the original data into train and test\n",
    "plt.axvline(x='2007-12-01', color='black', linestyle='--')\n",
    "\n",
    "# showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bt4y7zusBv5b"
   },
   "outputs": [],
   "source": [
    "# Calculate the rolling mean and standard deviation for a window of 12 observations\n",
    "rolmean=train_data.rolling(window=12).mean()\n",
    "rolstd=train_data.rolling(window=12).std()\n",
    "\n",
    "# Visualize the rolling mean and standard deviation\n",
    "plt.figure(figsize=(16,8))\n",
    "actual = plt.plot(train_data, color='cyan', label='Actual Series')\n",
    "rollingmean = plt.plot(rolmean, color='red', label='Rolling Mean') \n",
    "rollingstd = plt.plot(rolstd, color='green', label='Rolling Std. Dev.')\n",
    "plt.title('Rolling Mean & Standard Deviation of the Series')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnwqnEeDDxUh"
   },
   "source": [
    "#### **Observations and Insights: ____**\n",
    "1. Series has upward trend, it is not stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kbkBI8HEFJe"
   },
   "source": [
    "### **Test the stationarity using the Augmented Dickey-Fuller Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQr0yVIKlAp5"
   },
   "source": [
    "Use the **Augmented Dickey-Fuller (ADF) Test** to verify if the series is stationary or not. The null and alternate hypotheses for the ADF Test are defined as:\n",
    "\n",
    "**- Null hypothesis:** The Time Series is non-stationary\n",
    "\n",
    "\n",
    "**- Alternative hypothesis:** The Time Series is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to use adfuller test\n",
    "def adfuller(train_data):\n",
    "  #Importing adfuller using statsmodels\n",
    "  from statsmodels.tsa.stattools import adfuller\n",
    "  print('Dickey-Fuller Test: ')\n",
    "  adftest = adfuller(train_data['Value'])\n",
    "  adfoutput = pd.Series(adftest[0:4], index=['Test Statistic','p-value','Lags Used','No. of Observations'])\n",
    "  for key,value in adftest[4].items():\n",
    "    adfoutput['Critical Value (%s)'%key] = value\n",
    "  print(adfoutput)\n",
    "adfuller(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjwqnZVGJnC8"
   },
   "source": [
    "- **Observations and Insights**\n",
    " \n",
    "Observations:\n",
    "\n",
    "1. From the above test, we can see that the p-value = 0.995 i.e. > 0.05 (For 95% confidence intervals) therefore, we fail to reject the null hypothesis.\n",
    "2. Hence, we can confirm that the series is non-stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu0r1kYWJ1rO"
   },
   "source": [
    "###**Transformation of the dataset into a stationary one**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9Wv-ufGnBRp"
   },
   "source": [
    "**We can use some of the following methods to convert a non-stationary series into a stationary one:**\n",
    "\n",
    "\n",
    "1. Log Transformation\n",
    "2. Differencing the series (lagged series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOW6NcU-VN02"
   },
   "source": [
    "We take the average of ‘k’ consecutive values depending on the frequency of time series (in this capstone 12 months). \n",
    "\n",
    "Here, we will take the average over the past 1 year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fE2MvqLJqi6"
   },
   "outputs": [],
   "source": [
    " # Visualize the rolling mean and standard deviation after using log transformation\n",
    "plt.figure(figsize=(16,8))\n",
    "df_log = np.log(train_data) #Reduced variance\n",
    "MAvg = df_log.rolling(window=12).mean()\n",
    "MStd = df_log.rolling(window=12).std()\n",
    "plt.plot(df_log)\n",
    "plt.plot(MAvg, color='r', label = 'Moving Average')\n",
    "plt.plot(MStd, color='g', label = 'Standard Deviation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25V8-kvKV7hN"
   },
   "source": [
    "**Observations and Insights: _____**\n",
    "- Since **we can still see the upward trend in the series**, we can conclude that **the series is still non-stationary.** \n",
    "- However, the standard deviation is almost constant which implies that **now the series has constant variance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlQacDVzpfrJ"
   },
   "source": [
    "**Visualize the rolling mean and rolling standard deviation of the shifted series (df_shift) and check the stationarity by calling the adfuller() function. Also, write your observations on the same.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "df_shift = df_log - df_log.shift(periods = 1,fill_value=0)\n",
    "MAvg_shift = df_shift.rolling(window=12).mean()\n",
    "MStd_shift = df_shift.rolling(window=12).std()\n",
    "plt.plot(df_shift, color='c')\n",
    "#plt.plot(MAvg_shift, color='red', label = 'Moving Average')\n",
    "#plt.plot(MStd_shift, color='green', label = 'Standard Deviation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Dropping the null values that we get after applying differencing method\n",
    "df_shift = df_shift.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations and Insights:___**\n",
    "1.Since we can no longer see an upward trend, the series seems to be alomonst constant (stationary)\n",
    "\n",
    "2. The standard deviation also seems to be almost constant\n",
    "\n",
    "Lets verify using Augmented Dickey-Fuller (ADF) Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gD_Haj9zqD-M"
   },
   "outputs": [],
   "source": [
    "adfuller(df_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations and Insights: _____**\n",
    "1. From the above test, we can see that the p-value = 5.447548e-14 i.e. < 0.05 (For 95% confidence intervals) therefore, we can reject the null hypothesis.\n",
    "2. Hence, we can confirm that the series is now stationary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining stationarity through differencing##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "# implementing ADF test on the original time series data\n",
    "result = adfuller(train_data['Value'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "train_data.plot(ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# printing the results\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking seasonal differencing of the timeseries\n",
    "train_data_stationary = train_data.diff(periods=12).dropna()\n",
    "\n",
    "# implementing ADF test on the first order differenced time series data\n",
    "result = adfuller(train_data_stationary['Value'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "train_data_stationary.plot(ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# printing the results\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking 1st order differencing of the timeseries\n",
    "train_data_stationary = train_data.diff().dropna()\n",
    "\n",
    "# implementing ADF test on the first order differenced time series data\n",
    "result = adfuller(train_data_stationary['Value'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "train_data_stationary.plot(ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# printing the results\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHIOPvYWqMe4"
   },
   "source": [
    "**Observations and Insights: _____**\n",
    "1. p-value=0.00062<0.05. Stationarity is obtained by 1st order differencing. Therefore d=1 in ARIMA modelling\n",
    "\n",
    "Let's decompose the time series to check its different components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtttBdemiIxY"
   },
   "source": [
    "### **Elimination of trend and seasonality: Decomposition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBBWQmpLh_ir"
   },
   "outputs": [],
   "source": [
    "#Importing the seasonal_decompose function to decompose the time series\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomp = seasonal_decompose(train_data)\n",
    "\n",
    "trend = decomp.trend\n",
    "seasonal = decomp.seasonal\n",
    "residual = decomp.resid\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(411)\n",
    "plt.plot(train_data, label='Actual', marker='.')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend', marker='.')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal, label='Seasonality', marker='.')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals', marker='.')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWUmcqeyjQff"
   },
   "source": [
    "**Observations and Insights: ____**\n",
    "- We can see that there are significant **trend, seasonality and residuals components** in the series\n",
    "- The plot for seasonality shows that **Natural gas based CO2 emissions spike in July and August.**\n",
    "\n",
    "**Now let's move on to the model building section. First, we will plot the `ACF` and `PACF` plots to get the values of p and q i.e. order of AR and MA models to be used.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iv1j-3V3rM92"
   },
   "source": [
    "**Plot the auto-correlation function and partial auto-correlation function to get p and q values for AR, MA, ARMA, and ARIMA models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtstH0CvKVMX"
   },
   "source": [
    "### **Find optimal parameters (P, Q) and build the AR, MA, ARMA & ARIMA models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umZ3XhCyK40g"
   },
   "source": [
    "**Plot the ACF and PACF charts and find the optimal parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igReFApvKmhu"
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "\n",
    "plt.figure(figsize = (16,8))\n",
    "plot_acf(df_shift, lags = 24) \n",
    "plt.show() \n",
    "plot_pacf(df_shift, lags = 24) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXp8sqECya6e"
   },
   "source": [
    "**Observations and Insights: _____**\n",
    "\n",
    "**Observations:**\n",
    "- From the above PACF plot we can see that **the lag** at which the plot extends beyond the statistically significant boundary for the first time is **lag 1.** \n",
    "- This indicates that an **AR Model of lag 1 (p=1)** should be sufficient to fit the data.\n",
    "- Similarly, from the ACF plot, we can infer that **q=1.**\n",
    "- The ACF and PACF also capture the seasonality in the data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Axk38bXXNcS"
   },
   "source": [
    "###**AR Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWfeMfKJXZDA"
   },
   "source": [
    "Order p is the lag value after which the PACF plot crosses the upper confidence interval for the first time. These p lags will act as our features while forecasting the AR time series.\n",
    "\n",
    "Fit and predict the shifted series with the AR Model and calculate the RMSE. Also, visualize the time series and write your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "soyaDVNSXJGd"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "model_AR = AutoReg(df_shift.astype(float), lags=1)#Use number of lags as 1 and apply AutoReg function on df_shift series\n",
    "results_AR = model_AR.fit() #fit the model\n",
    "plt.plot(df_shift)\n",
    "predict = results_AR.predict() #predict the series \n",
    "predict = predict.fillna(0) #Converting NaN values to 0\n",
    "plt.plot(predict, color='red')\n",
    "plt.title('AR Model - RMSE: %.4f'% mean_squared_error(predict,df_shift['Value'], squared=False))  #Calculating rmse\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_efov_VOz-Ym"
   },
   "source": [
    "**Observations & Insights: _____**\n",
    "**Observations:________________________**\n",
    "1. The RMSE value is 0.2. It is very less. Therefore,this is a good model if we only want to use the AR component while modeling.\n",
    "2.The time series appears stationary\n",
    "\n",
    "**Let's check the AIC value** of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_AR.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build MA, ARMA, and ARIMA models as well, and see if we can get a better model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dkTbhOcXmK0"
   },
   "source": [
    "###**MA Model**\n",
    "\n",
    "####**Think about it:**\n",
    "\n",
    "- Do we really have to find AR & I value other than 0 to forecast on the MA based model?\n",
    "\n",
    "Order q of the MA process is obtained from the ACF plot, this is the lag after which ACF crosses the upper confidence interval for the first time.\n",
    "\n",
    "Fit and predict the shifted series with the MA Model and calculate the RMSE. Also, visualize the time series and write your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRYAByWs2sRd"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "model_MA =ARIMA(df_shift.astype(float), order=(0,0,1))#Using p=0, d=0, q=1 and apply ARIMA function on df_shift series\n",
    "results_MA = model_MA.fit()#fit the model\n",
    "plt.plot(df_shift)\n",
    "predict1 = results_MA.predict() #predict the series \n",
    "predict1 = predict1.fillna(0) \n",
    "plt.plot(predict, color='red')\n",
    "plt.title('MA Model - RMSE: %.4f'% mean_squared_error(results_MA.fittedvalues,df_shift['Value'], squared=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obBvUOML2h4Y"
   },
   "source": [
    "**Observations & Insights: _____**\n",
    "**Observations:________________________**\n",
    "1. THE RMSE is 0.2. Similar to AutoReg AR model. We need to check AIC value to determine if MA model is better than AR model for forecasting\n",
    "2. The time series appears stationary\n",
    "\n",
    "Let's check the AIC value of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_MA.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The MA model is giving higher AIC** when compared to the AR model, implying that **the AR model fits the training data better.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFyz2o8nZpRq"
   },
   "source": [
    "###**ARMA MODEL**\n",
    "\n",
    "**We will be using the above AR lag(P) & MA lag(Q) as a paramter** and d=0 in ARIMA so that it will work as an ARMA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iQFk_Sp3eJk"
   },
   "source": [
    "Fit and predict the shifted series with the ARMA Model and calculate the RMSE. Also, visualize the time series and write your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRI8fBNp3ySp"
   },
   "outputs": [],
   "source": [
    "#Code here\n",
    "plt.figure(figsize=(16,8))\n",
    "model_ARMA =ARIMA(df_shift.astype(float), order=(1,0,1)) #Using p=1, d=0, q=1 and apply ARIMA function on df_shift series\n",
    "results_ARMA =model_MA.fit() #fit the model\n",
    "plt.plot(df_shift)\n",
    "predict2 = results_MA.predict() #predict the series \n",
    "predict2 = predict2.fillna(0) \n",
    "plt.plot(predict, color='red')\n",
    "plt.title('ARMA Model - RMSE: %.4f'% mean_squared_error(results_ARMA.fittedvalues,df_shift['Value'], squared=False))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgUb1Bu-3126"
   },
   "source": [
    "**Observations & Insights: _____**\n",
    "**Observations:**\n",
    "1. THE RMSE is 0.0902. Similar to AutoReg AR model and MA(0,0,1) model\n",
    "2.The time series appears stationary.\n",
    "\n",
    "\n",
    "**Let's check the AIC value** of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfEpC1JO4EF6"
   },
   "source": [
    "**Check the AIC value of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgFFvTHV4Kfi"
   },
   "outputs": [],
   "source": [
    "#Code here\n",
    "results_ARMA.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The AIC value of the ARMA model is more or less similar** to MA model \n",
    "\n",
    "**Let us try using the ARIMA Model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hY6TyAS7Pkj"
   },
   "source": [
    "###**ARIMA MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EE2roq1G7JE-"
   },
   "source": [
    "**Fit and predict the shifted series with the ARIMA Model and calculate the RMSE. Also, visualize the time series and write your observations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNrJEOj631dx"
   },
   "outputs": [],
   "source": [
    "#Code here\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "model_ARIMA = ARIMA(df_log.astype(float), order=(1,1,1))#Using p=1, d=1, q=1 and apply ARIMA function on df_log series\n",
    "results_ARIMA = model_ARIMA.fit()#fit the model\n",
    "plt.plot(df_log)\n",
    "predict3 = results_ARIMA.predict() #predict the series \n",
    "predict3 = predict3.fillna(0)\n",
    "plt.plot(predict3, color='red')\n",
    "plt.title('ARIMA Model - RMSE: %.4f'% mean_squared_error(results_ARIMA.fittedvalues,df_log['Value'], squared=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:________**\n",
    "1. The RMSE value is 0.1965, lesser than previous models. The predicted values fit the data very well\n",
    "\n",
    "**Let's check the AIC value** of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hc2Rp2kRBEEZ"
   },
   "source": [
    "**Check the AIC value of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNy6tR1xBAqB"
   },
   "outputs": [],
   "source": [
    "results_ARIMA.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis, we can see that the ARIMA(1, 1, 1) is the best model as compared to others, as it has less RMSE as compared to all the other models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIhA2bboDxiD"
   },
   "source": [
    "###**Inverse Transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPWdZT6cD_g2"
   },
   "source": [
    "**Use the correct inverse transformation depending on the model chosen to get back the original values.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiWpP2oAFFyI"
   },
   "source": [
    "**Apply an inverse transformation on the predictions of the chosen model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPuxuXCNFyfF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing the fitted values\n",
    "predictions=pd.Series(results_ARIMA.fittedvalues)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Third step - applying exponential transformation\n",
    "predictions_ARIMA = np.exp(predictions)#use exponential function\n",
    "predictions_ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ta42tZUF0BL"
   },
   "source": [
    "**Plot the original vs predicted series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KeVCiA6GGfJ"
   },
   "outputs": [],
   "source": [
    "#Code here\n",
    "#Plotting the original vs predicted series\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(train_data, color = 'c', label = 'Original Series')  #plot the original train series\n",
    "plt.plot(predictions_ARIMA, color = 'r', label = 'Predicted Series')  #plot the predictions_ARIMA \n",
    "plt.title('Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2vbDPIqGIIy"
   },
   "source": [
    "**Observations & Insights: _____**\n",
    " We can see that **the predicted series is very similar to the original series** i.e. The model is good at predicting values on the training data.\n",
    "- Let us **forecast the closing prices for the next 24 months.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeOlfz-7GR4F"
   },
   "source": [
    "###**Forecast the values for next 24 months and compare it with test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpUdqIZwGsf4"
   },
   "outputs": [],
   "source": [
    "#Add the code blocks based on the requirements\n",
    "forecasted_ARIMA = results_ARIMA.forecast(steps=24)#forecast using the results_ARIMA for next 24 months. Keep steps=24\n",
    "forecasted_ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = forecasted_ARIMA.tolist()\n",
    "series1 = pd.Series(list1)\n",
    "series1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqZSSLo4izl6"
   },
   "outputs": [],
   "source": [
    "index = pd.date_range('2007-12-01','2009-12-01' , freq='1M')- pd.offsets.MonthBegin(1)\n",
    "df1 = pd.DataFrame()\n",
    "df1['forecasted'] = np.exp(series1)\n",
    "df1.index = index\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the original vs predicted series\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(train_data, color = 'c', label = 'Original Series')\n",
    "plt.plot(predictions_ARIMA, color = 'r', label = 'Prediction on Train data') #plot the predictions_ARIMA series\n",
    "plt.plot(df1, label = 'Forecast', color='b')  #plot the forecasted_ARIMA series\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- **As observed earlier, most of the predicted values on the training data are very close to the actual values** \n",
    "- **The model does not capture seasonalities in the data**, therefore the forecasted values are not able to identify these seasonalities and therefore not close to the actual data\n",
    "\n",
    "Let's test the RMSE of the transformed predictions and the original value on the training and testing data to check whether the model is giving a generalized performance or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "error =np.sqrt(mean_squared_error(predictions_ARIMA, train_data)) #calculate RMSE using the predictions_ARIMA and df_train \n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "error = np.sqrt(mean_squared_error(forecasted_ARIMA, test_data.iloc[0:24,:]))\n",
    "error#calculate RMSE using the forecasted_ARIMA and df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJQp6IG1ilay"
   },
   "source": [
    "####**Think about it:**\n",
    "- Can we use other than RMSE measurement to check the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6plS_7NNbwyr"
   },
   "source": [
    "####**Think about it:**\n",
    "\n",
    " Can we use other forecasting methods such as SARIMA to improve our model performance?\n",
    "\n",
    "- [A Gentle Introduction to SARIMA for Time Series Forecasting in Python](https://machinelearningmastery.com/sarima-for-time-series-forecasting-in-python/)\n",
    "- [Forecasting with Seasonal ARIMA in Python](https://www.datasciencecentral.com/profiles/blogs/tutorial-forecasting-with-seasonal-arima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frIDzCLQUvll"
   },
   "source": [
    "## **Proposed Approach**\n",
    "\n",
    "####**Refined insights**:\n",
    "- What are the most meaningful insights from the data relevant to the problem?\n",
    "1. The CO2 emissions due to Natural gas have an increasing trend. The time series also consists of seasonalities (with peak emissions in the months of July, Sepetember) and residuals\n",
    "\n",
    "####**Comparison of various techniques and their relative performance**:\n",
    "- How do different techniques perform? Which one is performing relatively better? Is there scope to improve the performance further?\n",
    "1. The ARIMA (1,1,1) fits better than all models. The RMSE value is lower than AR,MA and ARMA models. However, AIC values of MA and ARMA models are lower than all models\n",
    "2. ARIMA model could not forecast CO2 emissions taking the seasonalities into account. THerfore, SARIMA would be better at identifying seasonalities in the forecasted values\n",
    "\n",
    "####**Proposal for the final solution design**:\n",
    "- What model do you propose to be adopted? Why is this the best solution to adopt?\n",
    "1. I would obtain ACF and PACF plots of the stationary data (After turning non-stationary data to stationary)\n",
    "2. I would obtain p and q values from the PACF and ACF plots respectively\n",
    "3. I would use SARIMA model to fit the training data and forecast the next 24 months. This model should be able to identify seasonalities in the forecasted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the p, d and q parameters to take any value between 0 and 3\n",
    "p = d = q = range(0, 2)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "print('Examples of parameter combinations for Seasonal ARIMA: ')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determing p,d,q combinations with AIC scores.\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        mod = sm.tsa.statespace.SARIMAX(df_log.astype(float),\n",
    "                                        order=param,\n",
    "                                        seasonal_order=param_seasonal,\n",
    "                                        enforce_stationarity=False,enforce_invertibility=False)\n",
    "\n",
    "        results = mod.fit(disp=0)\n",
    "\n",
    "        print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.tsa.statespace.SARIMAX(df_log.astype(float),\n",
    "                                order=(1, 1, 1),\n",
    "                                seasonal_order=(1, 0, 1, 12),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "results = mod.fit(disp=0)\n",
    "\n",
    "print(round(results.aic,2))\n",
    "print(results.summary().tables[1])\n",
    "results.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the fitted values\n",
    "predictions=pd.Series(results.fittedvalues)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Third step - applying exponential transformation\n",
    "predictions_SARIMA = np.exp(predictions)#use exponential function\n",
    "predictions_SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here\n",
    "#Plotting the original vs predicted series\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(train_data, color = 'c', label = 'Original Series')  #plot the original train series\n",
    "plt.plot(predictions_SARIMA, color = 'r', label = 'Predicted Series')  #plot the predictions_ARIMA \n",
    "plt.title('Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the code blocks based on the requirements\n",
    "forecasted_SARIMA = results.forecast(steps=24)#forecast using the results_ARIMA for next 24 months. Keep steps=24\n",
    "forecasted_SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2 = forecasted_SARIMA.tolist()\n",
    "series2 = pd.Series(list2)\n",
    "series2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.date_range('2007-12-01','2009-12-01' , freq='1M')- pd.offsets.MonthBegin(1)\n",
    "df2 = pd.DataFrame()\n",
    "df2['forecasted'] = np.exp(series2)\n",
    "df2.index = index\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the original vs predicted series\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(train_data, color = 'c', label = 'Original Series')\n",
    "plt.plot(predictions_SARIMA, color = 'r', label = 'Prediction on Train data') #plot the predictions_ARIMA series\n",
    "plt.plot(df2, label = 'Forecast', color='b')  #plot the forecasted_ARIMA series\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_Axk38bXXNcS",
    "9hY6TyAS7Pkj",
    "qIhA2bboDxiD"
   ],
   "name": "Reference_Notebook_Milestone_2_Time_Series.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
